Hadoop operator.
The entry point for running various kinds of Hadoop jobs.

c
BEGIN {defshort '/H', defdsp 'hadoopalt', 'hadoop job dispatch table'}

sub hadoop_name() {$ENV{NI_HADOOP} || 'hadoop'}

Streaming.
We need to be able to find the Streaming jar, which is slightly nontrivial. The
hadoop docs suggest that $HADOOP_HOME has something to do with it, but I've
seen installations that had no such environment variable and everything worked
fine. Here's what we can do:

| 1. Use $NI_HADOOP_STREAMING_JAR if it's set
  2. Use `locate hadoop-streaming*.jar` if we have `locate`
  3. Use `find /usr /opt -name hadoop-streaming*.jar`, see if it's there

If those don't work, then we are officially SOL and you'll have to set
NI_HADOOP_STREAMING_JAR.

sub hadoop_streaming_jar {
  local $SIG{CHLD} = 'DEFAULT';
  $ENV{NI_HADOOP_STREAMING_JAR}
  || (split /\n/, `locate 'hadoop-streaming*.jar' \\
                   || find /usr -name 'hadoop-streaming*.jar' \\
                   || find /opt -name 'hadoop-streaming*.jar'`)[0]
  || die "ni: cannot find hadoop streaming jar "
       . "(you can fix this by setting \$NI_HADOOP_STREAMING_JAR)";
}

Input type autodetection.
Technically, hadoop operators take one or more HFDS input paths on stdin -- but
of course ni isn't going to just give up if we appear to have something else.
If we have something that obviously isn't an HDFS path, we upload that stream
into a temporary HDFS location and run against that.

sub hdfs_input_path {
  local $_;
  my $n;
  die "ni: hdfs_input_path: no data" unless $n = saferead \*STDIN, $_, 8192;
  if (/^hdfst?:\/\//) {
    $n = saferead \*STDIN, $_, 8192, length while $n;
    s/^hdfst:/hdfs:/gm;
    (0, grep length, split /\n/);
  } else {
    my $hdfs_tmp    = resource_tmp 'hdfs://';
    my $hdfs_writer = resource_write $hdfs_tmp;
    safewrite $hdfs_writer, $_;
    safewrite $hdfs_writer, $_ while saferead \*STDIN, $_, 8192;
    close $hdfs_writer;
    $hdfs_writer->await;
    (1, $hdfs_tmp);
  }
}

sub hadoop_lambda_file($$) {
  my ($name, $lambda) = @_;
  my $tmp = resource_tmp('file://') . $name;
  my $w   = resource_write $tmp;
  my ($stdin, @exec) = sni_exec_list @$lambda;
  safewrite $w, $stdin;
  close $w;
  ($tmp, @exec);
}

sub hadoop_embedded_cmd($@) {
  "sh -c " . shell_quote("cat " . shell_quote($_[0]) . " - | " . shell_quote(@_[1..$#_]));
}

defoperator hadoop_streaming => q{
  my ($map, $combine, $reduce) = @_;
  my ($nuke_inputs, @ipath) = hdfs_input_path;
  my $opath = resource_tmp "hdfs://";
  my ($mapper, @map_cmd) = hadoop_lambda_file 'mapper', $map;
  my ($combiner, @combine_cmd) = $combine
    ? hadoop_lambda_file 'combiner', $combine : ();
  my ($reducer, @reduce_cmd) = $reduce
    ? hadoop_lambda_file 'reducer', $reduce : ();

  my $streaming_jar = hadoop_streaming_jar;

  my $hadoop_fh = siproc {
    $mapper   =~ s|^file://||;
    $combiner =~ s|^file://|| if $combiner;
    $reducer  =~ s|^file://|| if $reducer;

    (my $mapper_file   = $mapper)         =~ s|.*/||;
    (my $combiner_file = $combiner || '') =~ s|.*/||;
    (my $reducer_file  = $reducer  || '') =~ s|.*/||;

    my $cmd = shell_quote
      hadoop_name,
      jar => $streaming_jar,
      -D  => "mapred.job.name=ni @ipath -> $opath",
      map((-input => $_), @ipath),
      -output => $opath,
      -file   => $mapper,
      -mapper => hadoop_embedded_cmd($mapper_file, @map_cmd),
      (defined $combiner
        ? (-file     => $combiner,
           -combiner => hadoop_embedded_cmd($combiner_file, @combine_cmd))
        : ()),
      (defined $reducer
        ? (-file    => $reducer,
           -reducer => hadoop_embedded_cmd($reducer_file, @reduce_cmd))
        : ());
    sh "$cmd 1>&2";
  };

  close $hadoop_fh;
  die "ni: hadoop streaming failed" if $hadoop_fh->await;

  (my $result_path = $opath) =~ s/^hdfs:/hdfst:/;
  print "$result_path/part-*\n";

  if ($nuke_inputs) {resource_nuke $_ for @ipath}

  resource_nuke $mapper;
  resource_nuke $combiner if defined $combiner;
  resource_nuke $reducer  if defined $reducer;
};

use constant hadoop_streaming_lambda => palt pmap(q{undef}, prc '_'),
                                             pmap(q{[]},    prc ':'),
                                             _qfn;

defhadoopalt S => pmap q{hadoop_streaming_op @$_},
                  pseq pc hadoop_streaming_lambda,
                       pc hadoop_streaming_lambda,
                       pc hadoop_streaming_lambda;
